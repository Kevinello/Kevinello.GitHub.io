<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>LLM远不仅仅是Chat Model——LangChain基本概念与使用示例 | Kevinello</title><meta name="keywords" content="LLM,Prompt-Engineering,OpenAI,langchain"><meta name="author" content="Kevinello"><meta name="copyright" content="Kevinello"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言 一图胜千言，LangChain已经成为当前LLM应用框架的事实标准，这篇文章就来对LangChain基本概念以及其具体使用场景做一个整理   LangChain是什么 LangChain是一个基于大语言模型的应用开发框架，它主要通过两种方式规范和简化了使用LLM的方式:  集成：集成外部数据(如文件、其他应用、API数据等)到LLM中 Agent：允许LLM通过决策与特定的环境交互，并由L"><meta property="og:type" content="article"><meta property="og:title" content="LLM远不仅仅是Chat Model——LangChain基本概念与使用示例"><meta property="og:url" content="http://kevinello.ltd/2023/09/06/LLM%E8%BF%9C%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AFChat%20Model%E2%80%94%E2%80%94LangChain%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/index.html"><meta property="og:site_name" content="Kevinello"><meta property="og:description" content="前言 一图胜千言，LangChain已经成为当前LLM应用框架的事实标准，这篇文章就来对LangChain基本概念以及其具体使用场景做一个整理   LangChain是什么 LangChain是一个基于大语言模型的应用开发框架，它主要通过两种方式规范和简化了使用LLM的方式:  集成：集成外部数据(如文件、其他应用、API数据等)到LLM中 Agent：允许LLM通过决策与特定的环境交互，并由L"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071425723-3c7e69.png"><meta property="article:published_time" content="2023-09-05T16:08:35.000Z"><meta property="article:modified_time" content="2023-09-13T03:14:43.124Z"><meta property="article:author" content="Kevinello"><meta property="article:tag" content="LLM"><meta property="article:tag" content="Prompt-Engineering"><meta property="article:tag" content="OpenAI"><meta property="article:tag" content="langchain"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071425723-3c7e69.png"><link rel="shortcut icon" href="/imgs/K.jpg"><link rel="canonical" href="http://kevinello.ltd/2023/09/06/LLM%E8%BF%9C%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AFChat%20Model%E2%80%94%E2%80%94LangChain%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload='this.media="all"'><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?d2a20aecba22b2eaf60183c4831d9a52";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-RV8K5FBVX5"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-RV8K5FBVX5")</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"13UJR6CRNO","apiKey":"456e56f51ec27a1e13d67bef144f6747","indexName":"Kevinello_blog","hits":{"per_page":6},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":180,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"LLM远不仅仅是Chat Model——LangChain基本概念与使用示例",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2023-09-13 11:14:43"}</script><noscript><style>#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="//at.alicdn.com/t/font_2232093_k6128tldgy.css"><meta name="generator" content="Hexo 5.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2022/04/11/myself-e3fde6.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">97</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071425723-3c7e69.png)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kevinello</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i> <span>Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i> <span>About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLM远不仅仅是Chat Model——LangChain基本概念与使用示例</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-05T16:08:35.000Z" title="发表于 2023-09-06 00:08:35">2023-09-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-13T03:14:43.124Z" title="更新于 2023-09-13 11:14:43">2023-09-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%8A%80%E6%9C%AF%E6%96%87%E7%AB%A0/">技术文章</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>29分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title="LLM远不仅仅是Chat Model——LangChain基本概念与使用示例"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前言"><a class="markdownIt-Anchor" href="#前言"></a> 前言</h1><p>一图胜千言，<code>LangChain</code>已经成为当前LLM应用框架的事实标准，这篇文章就来对LangChain基本概念以及其具体使用场景做一个整理</p><p><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171529861-e2f379.png" alt="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171529861-e2f379.png"></p><h1 id="langchain是什么"><a class="markdownIt-Anchor" href="#langchain是什么"></a> LangChain是什么</h1><p><code>LangChain</code>是一个基于大语言模型的应用开发框架，它主要通过两种方式规范和简化了使用<code>LLM</code>的方式:</p><ol><li><strong>集成</strong>：集成外部数据(如文件、其他应用、API数据等)到<code>LLM</code>中</li><li><strong>Agent</strong>：允许<code>LLM</code>通过决策与特定的环境交互，并由<code>LLM</code>协助决定下一步的操作</li></ol><p>LangChain的优点包括:</p><ol><li><strong>高度抽象的组件</strong>：规范和简化与语言模型交互所需的各种抽象和组件</li><li><strong>高度可自定义的Chains</strong>：提供了大量预置<code>Chains</code>的同时，支持自行继承BaseChain并实现相关逻辑以及各个阶段的<code>callback handler</code>等</li><li><strong>活跃的社区与生态</strong>：<code>Langchain</code>团队迭代速度非常快，能快速使用最新的语言模型特性，该团队也有langsmith, auto-evaluator等其它优秀项目，并且开源社区也有相当多的支持</li></ol><h1 id="langchain的主要组件"><a class="markdownIt-Anchor" href="#langchain的主要组件"></a> LangChain的主要组件</h1><p>这是一张<code>LangChain</code>的组件与架构图（<code>langchain python</code>和<code>langchain JS/TS</code>的架构基本一致，本文中以<code>langchain python</code>来完成相关介绍），基本完整描述了<code>LangChain</code>的组件与抽象层（<code>callback</code>不在这张图中，在下方我们会另外介绍），以及它们之间的相关联系</p><p><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171306709-f0c287.png" alt="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171306709-f0c287.png"></p><h2 id="model-io"><a class="markdownIt-Anchor" href="#model-io"></a> Model I/O</h2><p>首先我们从最基本面的部分讲起，Model I/O 指的是和LLM直接进行交互的过程</p><p><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171335161-5f4cf5.png" alt="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171335161-5f4cf5.png"></p><p>在Model I/O这一流程中，LangChain抽象的组件主要有三个：</p><ul><li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/model_io/models/">Language models</a></li><li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/model_io/prompts/">Prompts</a></li><li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/model_io/output_parsers/">Output parsers</a></li></ul><p>下面我们展开介绍一下</p><p>⚠️注：下面涉及的所有代码示例中的<code>OPENAI_API_KEY</code>和<code>OPENAI_BASE_URL</code>需要提前配置好，<code>OPENAI_API_KEY</code>指<strong>OpenAI/OpenAI代理服务</strong>的<code>API Key</code>，<code>OPENAI_BASE_URL</code>指OpenAI代理服务的<code>Base Url</code></p><h3 id="language-model"><a class="markdownIt-Anchor" href="#language-model"></a> Language Model</h3><p><code>Language Model</code>是真正与LLM / ChatModel进行交互的组件，它可以直接被当作普通的openai client来使用，在<code>LangChain</code>中，主要使用到的是<code>LLM</code>，<code>Chat Model</code>和<code>Embedding</code>三类Language Model</p><ul><li><p>LLM: 最基础的通过“<strong>text in ➡️ text out</strong>”模式来使用的Language Model，另一方面，LangChain也收录了大量的<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/llms/"><strong>第三方LLM</strong></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">llm = OpenAI(model_name=<span class="string">&quot;text-ada-001&quot;</span>, openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_BASE_URL)</span><br><span class="line"></span><br><span class="line">llm(<span class="string">&quot;What day comes after Friday?&quot;</span>)</span><br><span class="line"><span class="comment"># &#x27;\n\nSaturday.&#x27;</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Chat Model</strong>: <code>LLM</code>的变体，抽象了<code>Chat</code>这一场景下的使用模式，由“<strong>text in ➡️ text out</strong>”变成了“<strong>chat messages in ➡️ chat message out</strong>”，<code>chat message</code>是指<strong>text + message type(System, Human, AI)</strong></p><ul><li><strong>System</strong> - 告诉AI要做什么的背景信息上下文</li><li><strong>Human</strong> - 标识用户传入的消息类型</li><li><strong>AI</strong> - 标识AI返回的消息类型</li></ul><p>以下是一个简单的<code>Chat Model</code>使用示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.schema <span class="keyword">import</span> HumanMessage, SystemMessage</span><br><span class="line"></span><br><span class="line">chat = ChatOpenAI(model_name=<span class="string">&quot;gpt-4-0613&quot;</span>, temperature=<span class="number">1</span>, openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_BASE_URL)</span><br><span class="line">chat(</span><br><span class="line">    [</span><br><span class="line">        SystemMessage(content=<span class="string">&quot;You are an expert on large language models and can answer any questions related to large language models.&quot;</span>),</span><br><span class="line">        HumanMessage(content=<span class="string">&quot;What’s the difference between Generic Language Models, Instruction Tuned Models and Dialog Tuned Models&quot;</span>)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># AIMessage(content=&#x27;Generic Language Models, Instruction-Tuned Models, and Dialog-Tuned Models are all various types of language models that have been trained according to different datasets and methodologies. They are used in different contexts according to their specific strengths. \n\n1. **Generic Language Models**: These models are trained on a broad range of internet text. They are typically not tuned to specific tasks and thus can be used across a wide variety of applications. GPT-3, used by OpenAI, is an example of a general language model.\n\n2. **Instruction-Tuned Models**: These are language models that are fine-tuned specifically to follow instructions given in a prompt. They are trained using a procedure called Reinforcement Learning from Human Feedback (RLHF), following an initial supervised fine-tuning which consists of human AI trainers providing conversations where they play both the user and an AI assistant. This model often includes comparison data - two or more model responses are ranked by quality.\n\n3. **Dialog-Tuned Models**: Like Instruction-Tuned Models, these models are also trained with reinforcement learning from human feedback, and especially shine in multi-turn conversations. However, these are specifically designed for dialog scenarios, resulting in an ability to maintain more coherent and context-aware conversations. \n\nIn essence, the difference among the three revolves around the breadth of their training and their specific use-cases. Generic Language Models are broad but may lack specificity; Instruction-Tuned Models are better at following specific instructions given in prompts; and Dialog-Tuned Models excel in carrying out more coherent and elongated dialogues.&#x27;, additional_kwargs=&#123;&#125;, example=False)</span></span><br></pre></td></tr></table></figure><p>另一方面，LangChain也收录了大量的**<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/chat/">第三方Chat Model</a>**</p></li><li><p>Embedding: <code>Embedding</code>将一段文字<strong>向量化</strong>为一个定长的向量，有了文本的向量化表示我们就可以做一些像语义搜索，聚类选择等来选择需要的文本片段，如下是将一个embed任意字符串的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_BASE_URL)</span><br><span class="line">text_embedding = embeddings.embed_query(<span class="string">&quot;To embed text(it can have any length)&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Your embedding&#x27;s length: <span class="subst">&#123;<span class="built_in">len</span>(text_embedding)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Here&#x27;s a sample: <span class="subst">&#123;text_embedding[:<span class="number">5</span>]&#125;</span>...&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Your embedding&#x27;s length: 1536</span></span><br><span class="line"><span class="string">Here&#x27;s a sample: [-0.03194352, 0.009228715, 0.00807182, 0.0077545005, 0.008256923]...</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="prompts"><a class="markdownIt-Anchor" href="#prompts"></a> Prompts</h3><p><strong>Prompt</strong>指用户的一系列指令和输入，是决定<code>Language Model</code>输出内容的唯一输入，主要用于帮助模型理解上下文并生成相关和连贯的输出，如回答问题、拓写句子和总结问题。在<code>LangChain</code>中的相关组件主要有<code>Prompt Template</code>和<code>Example selectors</code>，以及后面会提到的辅助/补充<strong>Prompt</strong>的一些其它组件</p><ul><li><p>Prompt Template: 预定义的一系列指令和输入参数的<code>prompt</code>模版，支持更加灵活的输入，如支持**output instruction(输出格式指令), partial input(提前指定部分输入参数), examples(输入输出示例)**等；<code>LangChain</code>提供了大量方法来创建<code>Prompt Template</code>，有了这一层组件就可以在不同<code>Language Model</code>和不同<code>Chain</code>下大量复用<code>Prompt Template</code>了，<code>Prompt Template</code>中也会有下面将提到的<code>Example selectors, Output Parser</code>的参与</p></li><li><p>Example selectors: 在很多场景下，单纯的<strong>instruction + input</strong>的<code>prompt</code>不足以让<code>LLM</code>完成高质量的推理回答，这时候我们就还需要为<code>prompt</code>补充一些针对具体问题的示例，LangChain将这一功能抽象为了<code>Example selectors</code>这一组件，我们可以基于关键字，相似度(通常使用<strong>MMR/cosine similarity/ngram</strong>来计算相似度, 在后面的向量数据库章节中会提到)。为了让最终的<code>prompt</code>不超过<code>Language Model</code>的token上限（各个模型的token上限见下表），<code>LangChain</code>还提供了<code>LengthBasedExampleSelector</code>，根据长度来限制example数量，对于较长的输入，它会选择包含较少示例的提示，而对于较短的输入，它会选择包含更多示例</p><table><thead><tr><th>Model</th><th>Max Tokens</th><th>Training Data</th></tr></thead><tbody><tr><td>gpt-4</td><td>8,192 tokens</td><td>Up to Sep 2021</td></tr><tr><td>gpt-4-0613</td><td>8,192 tokens</td><td>Up to Sep 2021</td></tr><tr><td>gpt-4-32k</td><td>32,768 tokens</td><td>Up to Sep 2021</td></tr><tr><td>gpt-4-32k-0613</td><td>32,768 tokens</td><td>Up to Sep 2021</td></tr><tr><td>gpt-3.5-turbo</td><td>4,096 tokens</td><td>Up to Sep 2021</td></tr><tr><td>gpt-3.5-turbo-16k</td><td>16,384 tokens</td><td>Up to Sep 2021</td></tr><tr><td>gpt-3.5-turbo-0613</td><td>4,096 tokens</td><td>Up to Sep 2021</td></tr><tr><td>gpt-3.5-turbo-16k-0613</td><td>16,384 tokens</td><td>Up to Sep 2021</td></tr><tr><td>text-davinci-003</td><td>4,097 tokens</td><td>Up to Jun 2021</td></tr><tr><td>text-davinci-002</td><td>4,097 tokens</td><td>Up to Jun 2021</td></tr><tr><td>code-davinci-002</td><td>8,001 tokens</td><td>Up to Jun 2021</td></tr></tbody></table></li></ul><h3 id="output-parser"><a class="markdownIt-Anchor" href="#output-parser"></a> Output Parser</h3><p>通常我们希望<code>Language Model</code>的输出是固定的格式，以支持我们解析其输出为结构化数据，<code>LangChain</code>将这一诉求所需的功能抽象成了<code>Output Parser</code>这一组件，并提供了一系列的预定义<code>Output Parser</code>，如最常用的<strong>Structured output parser, List parser</strong>，以及在<code>LLM</code>输出无法解析时发挥作用的<strong>Auto-fixing parser和Retry parser</strong></p><p><code>Output Parser</code>需要和<strong>Prompt Template, Chain</strong>组合使用：</p><ul><li>Prompt Template: 在<code>Prompt Template</code>中通过指定<code>partial_variables</code>为<code>Output Parser</code>的format，即可在<code>prompt</code>中补充让模型输出所需格式内容的指令</li><li>Chain: 在<code>Chain</code>中指定<code>Output Parser</code>，并使用<code>Chain</code>的<code>predict_and_parse / apply_and_parse</code>方法启动<code>Chain</code>，即可直接输出解析后的数据</li></ul><h3 id="使用示例"><a class="markdownIt-Anchor" href="#使用示例"></a> 使用示例</h3><p>以下是一个完整的组合<strong>Prompt Template, Output Parser和Chain</strong>的具体用例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0.5</span>, model_name=<span class="string">&quot;gpt-3.5-turbo-16k-0613&quot;</span>, openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_BASE_URL)</span><br><span class="line">template = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">## Input</span></span><br><span class="line"><span class="string">&#123;text&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## Instruction</span></span><br><span class="line"><span class="string">Please summarize the piece of text in the input part above.</span></span><br><span class="line"><span class="string">Respond in a manner that a 5 year old would understand.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;format_instructions&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">YOUR RESPONSE:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个Output Parser，包含两个输出字段，并指定类型和说明</span></span><br><span class="line">output_parser = StructuredOutputParser.from_response_schemas(</span><br><span class="line">    [</span><br><span class="line">        ResponseSchema(name=<span class="string">&quot;keywords&quot;</span>, <span class="built_in">type</span>=<span class="string">&quot;list&quot;</span>, description=<span class="string">&quot;keywords of the text&quot;</span>),</span><br><span class="line">        ResponseSchema(name=<span class="string">&quot;summary&quot;</span>, <span class="built_in">type</span>=<span class="string">&quot;string&quot;</span>, description=<span class="string">&quot;summary of the text&quot;</span>),</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 创建Prompt Template，并将format_instructions通过partial_variables直接指定为Output Parser的format</span></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    input_variables=[<span class="string">&quot;text&quot;</span>],</span><br><span class="line">    template=template,</span><br><span class="line">    partial_variables=&#123;<span class="string">&quot;format_instructions&quot;</span>: output_parser.get_format_instructions()&#125;,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 创建Chain并绑定Prompt Template和Output Parser(它将自动使用Output Parser解析llm输出)</span></span><br><span class="line">summarize_chain = LLMChain(llm=llm, verbose=<span class="literal">True</span>, prompt=prompt, output_parser=output_parser)</span><br><span class="line"></span><br><span class="line">to_summarize_text = <span class="string">&#x27;Abstract. Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large.&#x27;</span></span><br><span class="line">output = summarize_chain.predict(text=to_summarize_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="built_in">print</span> (json.dumps(output, indent=<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;keywords&quot;</span>: [</span><br><span class="line">        <span class="string">&quot;Text-to-SQL&quot;</span>,</span><br><span class="line">        <span class="string">&quot;SQL queries&quot;</span>,</span><br><span class="line">        <span class="string">&quot;natural language questions&quot;</span>,</span><br><span class="line">        <span class="string">&quot;databases&quot;</span>,</span><br><span class="line">        <span class="string">&quot;prompt learning&quot;</span>,</span><br><span class="line">        <span class="string">&quot;large language models&quot;</span>,</span><br><span class="line">        <span class="string">&quot;LLMs&quot;</span>,</span><br><span class="line">        <span class="string">&quot;SQL syntax requirements&quot;</span>,</span><br><span class="line">        <span class="string">&quot;demonstration examples&quot;</span>,</span><br><span class="line">        <span class="string">&quot;semantic gap&quot;</span></span><br><span class="line">    ],</span><br><span class="line">    <span class="attr">&quot;summary&quot;</span>: <span class="string">&quot;Text-to-SQL is a method that helps users generate SQL queries for their questions about databases. One approach is to use large language models to understand the question and generate the SQL. However, this approach faces challenges with strict SQL syntax rules. Existing methods use examples to teach the language models, but they struggle when the examples are very different from the question.&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="data-connection"><a class="markdownIt-Anchor" href="#data-connection"></a> Data connection</h2><p>正如我在文章开头的<strong>LangChain是什么</strong>一节中提到的，集成外部数据到Language Model中是LangChain提供的核心能力之一，也是市面上很多优秀的大语言模型应用成功的核心之一（<a target="_blank" rel="noopener" href="https://docs.github.com/en/copilot/github-copilot-chat/using-github-copilot-chat">Github Copilot Chat</a>，<a target="_blank" rel="noopener" href="https://chrome.google.com/webstore/detail/sider-chatgpt-sidebar-gpt/difoiogjjojoaoomphldepapgpbgkhkb">网页聊天助手</a>，<a target="_blank" rel="noopener" href="https://www.summarizepaper.com/">论文总结助手</a>，youtube视频总结助手…），在<code>LangChain</code>中，<code>Data connection</code>这一层主要包含以下四个抽象组件：</p><ul><li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/data_connection/document_loaders/">Document loaders</a></li><li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/data_connection/document_transformers/">Document transformers</a></li><li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/data_connection/vectorstores/">Vector stores</a></li><li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/modules/data_connection/retrievers/">Retrievers</a></li></ul><p><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171337433-d96fb1.png" alt="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171337433-d96fb1.png"></p><p>下面我们展开介绍一下</p><h3 id="document-loaders"><a class="markdownIt-Anchor" href="#document-loaders"></a> Document loaders</h3><p>为了补全LLM的上下文信息，给予其足够的提示，我们需要从各类数据源获取各类数据，这也就是<code>LangChain</code>抽象的<code>Document loaders</code>这一组件的功能</p><p>使用<code>Document loaders</code>可以将源中的数据加载为<code>Document</code>。<code>Document</code>由一段文本和相关元数据组成。例如，有用于加载简单.txt文件的，用于加载相对结构化的markdown文件的，用于加载任何网页文本内容，甚至用于加载解析YouTube视频的脚本</p><p>同时LangChain还收录了海量的<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/document_loaders/">第三方Document loaders</a>，以下是一个使用<code>NotionDBLoader</code>来加载<code>notion database</code>中的<code>page</code>为<code>Document</code>的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> NotionDBLoader</span><br><span class="line"><span class="keyword">from</span> getpass <span class="keyword">import</span> getpass</span><br><span class="line"></span><br><span class="line"><span class="comment"># getpass()指引用户输入密钥</span></span><br><span class="line">NOTION_TOKEN = getpass()</span><br><span class="line">DATABASE_ID = getpass()</span><br><span class="line"></span><br><span class="line">loader = NotionDBLoader(</span><br><span class="line">    integration_token=NOTION_TOKEN,</span><br><span class="line">    database_id=DATABASE_ID,</span><br><span class="line">    request_timeout_sec=<span class="number">30</span>,  <span class="comment"># optional, defaults to 10</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 请求详情见 https://developers.notion.com/reference/post-database-query</span></span><br><span class="line">docs = loader.load()</span><br><span class="line">docs[<span class="number">0</span>].page_content[:<span class="number">100</span>]</span><br></pre></td></tr></table></figure><h3 id="document-transformers"><a class="markdownIt-Anchor" href="#document-transformers"></a> Document transformers</h3><p>当我们加载Document到内存后，我们通常还会希望将他们尽可能的结构化 / 分块，以进行更加灵活的操作</p><p>最简单的例子是，我们很多时候都需要将一个长文档拆分成更小的块，以便放入模型的上下文窗口中；LangChain有许多内置的<strong>Document transformers</strong>(大部分都是<code>Text Spliter</code>)，可以轻松地拆分、合并、筛选和以其他方式操作文档，一些常用的<strong>Document transformers</strong>如下：</p><table><thead><tr><th>Document transformers</th><th>功能</th></tr></thead><tbody><tr><td>LatexTextSplitter</td><td>沿着Latex标题、标题、枚举等分割文本</td></tr><tr><td>MarkdownHeaderTextSplitter</td><td>沿着指定的Markdown的section header分割</td></tr><tr><td>NLTKTextSplitter</td><td>使用NLTK的分割器</td></tr><tr><td>PythonCodeTextSplitter</td><td>沿着Python类和方法的定义分割文本</td></tr><tr><td>RecursiveCharacterTextSplitter</td><td>用于通用文本的分割器。它以一个字符列表为参数，尽可能地把所有的段落（然后是句子，然后是单词）放在一起</td></tr><tr><td>SpacyTextSplitter</td><td>使用Spacy的分割器</td></tr><tr><td>TokenTextSplitter</td><td>根据openAI的token数进行分割</td></tr></tbody></table><p>同时LangChain也收录了很多<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/document_transformers/">第三方的<strong>Document transformers</strong></a>（如基于爬虫中常见的beautiful soup, 基于OpenAI打metadata tag的等等）</p><h3 id="vector-stores"><a class="markdownIt-Anchor" href="#vector-stores"></a> Vector stores</h3><p>在前面的Prompt一节中我们提到了Example selectors，那么我们要如何找到相关示例呢？通常这个答案就是向量数据库</p><p><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171409682-cc3134.png" alt="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171409682-cc3134.png"></p><p>存储和搜索非结构化数据的最常见方式之一是将其向量化(embedding)并存储所得到的嵌入向量，然后在查询时向量化非结构化查询并检索与嵌入的查询最相似的向量。<strong>Vector stores</strong>负责存储向量化数据并提供向量搜索的功能，常见的向量数据库包括FAISS, Milvus, Pinecone, Weaviate, Chroma等，日常使用更常用的是FAISS</p><p>同时LangChain也收录了很多<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/vectorstores/">第三方的Vector Stores</a>，提供更加强大的向量搜索等功能</p><h3 id="retrievers"><a class="markdownIt-Anchor" href="#retrievers"></a> Retrievers</h3><p><strong>Retrievers</strong>是LangChain提供的将<code>Document</code>与<code>Language Model</code>相结合的组件<br>LangChain中有许多不同类型的<strong><strong>Retrievers</strong></strong>，但最广泛使用的就是VectoreStoreRetriever，我们可以直接把它当做连接向量数据库和Language Model的中间层，并且VectoreStoreRetriever的使用也很简单，直接<code>retriever = db.as_retriever()</code>即可</p><p>当然我们还有很多其它的<strong><strong>Retrievers</strong></strong>如Web search Retrievers等，LangChain也收录了很多<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/integrations/retrievers/">第三方的<strong><strong>Retrievers</strong></strong></a></p><h3 id="使用示例-2"><a class="markdownIt-Anchor" href="#使用示例-2"></a> 使用示例</h3><p>以下就是一个使用向量数据库 + VectoreStoreRetriever + QA Chain的QA应用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chat_models <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings.openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化LLM</span></span><br><span class="line">llm = ChatOpenAI(temperature=<span class="number">0.5</span>, model_name=<span class="string">&quot;gpt-3.5-turbo-16k-0613&quot;</span>, openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_PROXY_URL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载文档</span></span><br><span class="line">loader = TextLoader(<span class="string">&#x27;path/to/related/document&#x27;</span>)</span><br><span class="line">doc = loader.load()</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;You have <span class="subst">&#123;<span class="built_in">len</span>(doc)&#125;</span> document&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;You have <span class="subst">&#123;<span class="built_in">len</span>(doc[<span class="number">0</span>].page_content)&#125;</span> characters in that document&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割字符串</span></span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">3000</span>, chunk_overlap=<span class="number">400</span>)</span><br><span class="line">docs = text_splitter.split_documents(doc)</span><br><span class="line">num_total_characters = <span class="built_in">sum</span>([<span class="built_in">len</span>(x.page_content) <span class="keyword">for</span> x <span class="keyword">in</span> docs])</span><br><span class="line"><span class="built_in">print</span> (<span class="string">f&quot;Now you have <span class="subst">&#123;<span class="built_in">len</span>(docs)&#125;</span> documents that have an average of <span class="subst">&#123;num_total_characters / <span class="built_in">len</span>(docs):,<span class="number">.0</span>f&#125;</span> characters (smaller pieces)&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化向量化模型</span></span><br><span class="line">embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_PROXY_URL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向量化Document并存入向量数据库（绑定向量和对应Document元数据），这里我们选择本地最常用的FAISS数据库</span></span><br><span class="line"><span class="comment"># 注意: 这会向OpenAI产生请求并产生费用</span></span><br><span class="line">doc_search = FAISS.from_documents(docs, embeddings)</span><br><span class="line"></span><br><span class="line">qa = RetrievalQA.from_chain_type(llm=llm, chain_type=<span class="string">&quot;stuff&quot;</span>, retriever=doc_search.as_retriever(), verbose=<span class="literal">True</span>)</span><br><span class="line">query = <span class="string">&quot;Specific questions to be asked&quot;</span></span><br><span class="line">qa.run(query)</span><br></pre></td></tr></table></figure><p>执行后verbose输出日志如下：</p><blockquote><p>You have 1 document<br>You have 74663 characters in that document<br>Now you have 29 documents that have an average of 2,930 characters (smaller pieces)</p><p><strong>&gt; Entering new chain…</strong></p><p>Prompt after formatting:</p><p><em><strong>System: Use the following pieces of context to answer the users question.<br>If you don’t know the answer, just say that you don’t know, don’t try to make up an answer.<br>----------------</strong></em></p><p><em><strong><retrived document content>…</retrived></strong></em></p><p><em><strong>Human: What does the author describe as good work?</strong></em></p><p><em><strong>The author describes working on things that aren’t prestigious as a sign of good work. They believe that working on unprestigious types of work can lead to the discovery of something real and that it indicates having the right kind of motives. The author also mentions that working on things that last, such as paintings, is considered good work.</strong></em></p></blockquote><h2 id="chains"><a class="markdownIt-Anchor" href="#chains"></a> Chains</h2><p>接下来就是LangChain中的主角——Chains了，Chains是LangChain中为连接多次与Language Model的交互过程而抽象的重要组件，它可以将多个组件组合在一起以创建一个单一的、连贯的任务，也可以嵌套多个Chain组合在一起，或者将Chain与其他组件组合来构建更复杂的Chain</p><p>除了单一Chain外，常用的几个Chains如下：</p><h3 id="router-chain"><a class="markdownIt-Anchor" href="#router-chain"></a> Router Chain</h3><p>在一些场景下，我们需要根据输入 / 上下文决定使用哪一条Chain，甚至哪一个Prompt，Router Chain就提供了诸如<a target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html">MultiPromptChain</a>, <a target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/chains/langchain.chains.router.llm_router.LLMRouterChain.html">LLMRouterChain</a>等一系列用于决策的Chain（这与后面我们会提到的Agent有类似的地方）</p><p>Router Chain由两部分组成：</p><ul><li>Router Chain本身：负责决定下一个目标Chain</li><li>目标Chains：Router Chain可以路由到的目标Chain</li></ul><h3 id="sequential-chain"><a class="markdownIt-Anchor" href="#sequential-chain"></a> Sequential Chain</h3><p>顾名思义，顺序执行的串行Chain，其中最简单的<strong><strong>SimpleSequentialChain</strong></strong>非常简单粗暴，<strong><strong>SimpleSequentialChain</strong></strong>的每个子Chain都有一个单一的输入/输出，并且一个步骤的输出是下一步的输入<br>而高阶一些的<strong><strong>SequentialChain</strong></strong>则允许多输入输出，并且我们可以通过添加后面会提到的Memory等来提高其推理表现</p><h3 id="map-reduce-chain"><a class="markdownIt-Anchor" href="#map-reduce-chain"></a> Map-reduce Chain</h3><p>Map-reduce Chain主要用于summary的场景，针对那些超长的文档，首先我们通过前面提到过的TextSpliter按一定规则分割文档为更小的Chunks（通常使用RecursiveCharacterTextSplitter，如果Document是结构化的可以考虑使用指定的TextSpliter）,然后对每个分割的部分执行”map-chain”，收集全部”map-chain”的输出后，再执行”reduce-chain”，获得最终的summary输出</p><p><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171453470-5cddb0.png" alt="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171453470-5cddb0.png"></p><h3 id="使用示例-3"><a class="markdownIt-Anchor" href="#使用示例-3"></a> 使用示例</h3><p>下面就是一个Router Chain中的<a target="_blank" rel="noopener" href="https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html">MultiPromptChain</a>的具体示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains.router <span class="keyword">import</span> MultiPromptChain</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> ConversationChain</span><br><span class="line"><span class="keyword">from</span> langchain.chains.llm <span class="keyword">import</span> LLMChain</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain.chains.router.llm_router <span class="keyword">import</span> LLMRouterChain, RouterOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain.chains.router.multi_prompt_prompt <span class="keyword">import</span> MULTI_PROMPT_ROUTER_TEMPLATE</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义要路由的prompts</span></span><br><span class="line">physics_template = <span class="string">&quot;&quot;&quot;You are a very smart physics professor. \</span></span><br><span class="line"><span class="string">You are great at answering questions about physics in a concise and easy to understand manner. \</span></span><br><span class="line"><span class="string">When you don&#x27;t know the answer to a question you admit that you don&#x27;t know.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Here is a question:</span></span><br><span class="line"><span class="string">&#123;input&#125;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">math_template = <span class="string">&quot;&quot;&quot;You are a very good mathematician. You are great at answering math questions. \</span></span><br><span class="line"><span class="string">You are so good because you are able to break down hard problems into their component parts, \</span></span><br><span class="line"><span class="string">answer the component parts, and then put them together to answer the broader question.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Here is a question:</span></span><br><span class="line"><span class="string">&#123;input&#125;&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 整理prompt和相关信息</span></span><br><span class="line">prompt_infos = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;physics&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;Good for answering questions about physics&quot;</span>,</span><br><span class="line">        <span class="string">&quot;prompt_template&quot;</span>: physics_template,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;math&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;Good for answering math questions&quot;</span>,</span><br><span class="line">        <span class="string">&quot;prompt_template&quot;</span>: math_template,</span><br><span class="line">    &#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0.5</span>, openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_PROXY_URL+<span class="string">&quot;/v1&quot;</span>)</span><br><span class="line">destination_chains = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> p_info <span class="keyword">in</span> prompt_infos:</span><br><span class="line">    <span class="comment"># 以每个prompt为基础创建一个destination_chain(开启verbose)</span></span><br><span class="line">    name = p_info[<span class="string">&quot;name&quot;</span>]</span><br><span class="line">    prompt_template = p_info[<span class="string">&quot;prompt_template&quot;</span>]</span><br><span class="line">    prompt = PromptTemplate(template=prompt_template, input_variables=[<span class="string">&quot;input&quot;</span>])</span><br><span class="line">    chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line">    destination_chains[name] = chain</span><br><span class="line"><span class="comment"># 创建一个缺省chain，如果没有其他chain满足路由条件，则使用该chain</span></span><br><span class="line">default_chain = ConversationChain(llm=llm, output_key=<span class="string">&quot;text&quot;</span>)</span><br><span class="line"></span><br><span class="line">destinations = [<span class="string">f&quot;<span class="subst">&#123;p[<span class="string">&#x27;name&#x27;</span>]&#125;</span>: <span class="subst">&#123;p[<span class="string">&#x27;description&#x27;</span>]&#125;</span>&quot;</span> <span class="keyword">for</span> p <span class="keyword">in</span> prompt_infos]</span><br><span class="line">destinations_str = <span class="string">&quot;\n&quot;</span>.join(destinations)</span><br><span class="line"><span class="comment"># 根据prompt_infos中的映射关系创建router_prompt</span></span><br><span class="line">router_template = MULTI_PROMPT_ROUTER_TEMPLATE.<span class="built_in">format</span>(destinations=destinations_str)</span><br><span class="line">router_prompt = PromptTemplate(</span><br><span class="line">    template=router_template,</span><br><span class="line">    input_variables=[<span class="string">&quot;input&quot;</span>],</span><br><span class="line">    output_parser=RouterOutputParser(),</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 创建router_chain(开启verbose)</span></span><br><span class="line">router_chain = LLMRouterChain(llm_chain=LLMChain(llm=llm, prompt=router_prompt, verbose=<span class="literal">True</span>), verbose=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将router_chain和destination_chains以及default_chain组合成MultiPromptChain(开启verbose)</span></span><br><span class="line">chain = MultiPromptChain(</span><br><span class="line">    router_chain=router_chain,</span><br><span class="line">    destination_chains=destination_chains,</span><br><span class="line">    default_chain=default_chain,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># run</span></span><br><span class="line">chain.run(<span class="string">&quot;What is black body radiation?&quot;</span>)</span><br></pre></td></tr></table></figure><p>执行后verbose输出日志如下：</p><blockquote><p><strong>Entering new chain…</strong><br>Prompt after formatting:<br>***Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.</p><p>&lt;&lt; FORMATTING &gt;&gt;<br>Return a markdown code snippet with a JSON object formatted to look like:</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;destination&quot;</span>: string \ name of the prompt to use or <span class="string">&quot;DEFAULT&quot;</span></span><br><span class="line">    <span class="string">&quot;next_inputs&quot;</span>: string \ a potentially modified version of the original input</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>REMEMBER: “destination” MUST be one of the candidate prompt names specified below OR it can be “DEFAULT” if the input is not well suited for any of the candidate prompts.<br>REMEMBER: “next_inputs” can just be the original input if you don’t think any modifications are needed.</p><p>&lt;&lt; CANDIDATE PROMPTS &gt;&gt;<br>physics: Good for answering questions about physics<br>math: Good for answering math questions</p><p>&lt;&lt; INPUT &gt;&gt;<br>What is black body radiation?</p><p>&lt;&lt; OUTPUT &gt;&gt;***</p><p><strong>&gt; Finished chain.</strong></p><p>physics: {‘input’: ‘What is black body radiation?’}</p><p><strong>&gt; Entering new chain…</strong></p><p>Prompt after formatting:</p><p>***You are a very smart physics professor. You are great at answering questions about physics in a concise and easy to understand manner. When you don’t know the answer to a question you admit that you don’t know.</p><p>Here is a question:<br>What is black body radiation?***</p><p><strong>&gt; Finished chain.</strong></p></blockquote><h2 id="memory"><a class="markdownIt-Anchor" href="#memory"></a> Memory</h2><p><code>Memory</code>可以帮助<code>Language Model</code>补充历史信息的上下文，<code>LangChain</code>中的<code>Memory</code>是一个有点模糊的术语，它可以像记住你过去聊天过的信息一样简单，也可以结合向量数据库做更加复杂的历史信息检索，甚至维护相关实体及其关系的具体信息，这取决于具体的应用</p><p>通常Memory用于较长的Chain，能一定程度上提高模型的推理表现</p><p><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171459538-a72d84.png" alt="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171459538-a72d84.png"></p><p>常用的Memory类型如下：</p><ul><li><strong>Chat Messages</strong><br>最简单Memory，将历史Chat记录作为补充信息放入prompt中</li><li><strong>Vector store-backed memory</strong><br>基于向量数据库的Memory，将memory存储在向量数据库中，并在每次调用时查询<strong>TopK的最“重要”的文档</strong><br>这与大多数其他内存类的不同之处在于，它不显式跟踪交互的顺序，最多基于部分元数据筛选一下向量数据库的查询范围</li><li><strong>Conversation buffer(window) memory</strong><br>保存一段时间内的历史Chat记录，它只使用<strong>最后K个记录（仅保持最近交互的滑动窗口）</strong>，这样buffer也就不会变得太大，避免超过token上限</li><li><strong>Conversation summary memory</strong><br>这种类型的Memory会随着Chat的进行创建对话的摘要，并将当前摘要存储在Memory中，用于后续对话的history提示；这种memory方案对长会话非常有用，<strong>但频繁的总结摘要会耗费大量的token</strong></li><li><strong>Conversation Summary Buffer Memory</strong><br>结合了<code>buffer memory</code>和<code>summary memory</code>的策略，依旧会在内存中保留最后的一些Chat记录作为buffer，并在buffer的总token数达到预置的上限后，<strong>对所有Chat记录总结摘要作为SystemMessage并清理其它历史Messages</strong>；这种memory方案结合了<code>buffer memory</code>和<code>summary memory</code>的优点，<strong>既不会频繁地总结摘要消耗token，也不会让buffer缺失过多信息</strong></li></ul><h3 id="使用示例-4"><a class="markdownIt-Anchor" href="#使用示例-4"></a> 使用示例</h3><p>下面是一个<code>Conversation Summary Buffer Memory</code>在<code>ConversationChain</code>中的使用示例，包含了<strong>切换会话时恢复现场memory</strong>的方法以及<strong>自定义summary prompt</strong>的方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> ConversationChain</span><br><span class="line"><span class="keyword">from</span> langchain.memory <span class="keyword">import</span> ConversationSummaryBufferMemory</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> langchain.schema <span class="keyword">import</span> SystemMessage, AIMessage, HumanMessage</span><br><span class="line"><span class="keyword">from</span> langchain.memory.prompt <span class="keyword">import</span> SUMMARY_PROMPT</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line">llm = OpenAI(temperature=<span class="number">0.7</span>, openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_PROXY_URL+<span class="string">&quot;/v1&quot;</span>)</span><br><span class="line"><span class="comment"># ConversationSummaryBufferMemory默认使用langchain.memory.prompt.SUMMARY_PROMPT作为summary的PromptTemplate</span></span><br><span class="line"><span class="comment"># 如果对它summary的格式/内容有特殊要求，可以自定义PromptTemplate（实测默认的summary有些流水账）</span></span><br><span class="line">prompt_template_str = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">## Instruction</span></span><br><span class="line"><span class="string">Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new concise and detailed summary.</span></span><br><span class="line"><span class="string">Don&#x27;t repeat the conversation directly in the summary, extract key information instead.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## EXAMPLE</span></span><br><span class="line"><span class="string">Current summary:</span></span><br><span class="line"><span class="string">The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">New lines of conversation:</span></span><br><span class="line"><span class="string">Human: Why do you think artificial intelligence is a force for good?</span></span><br><span class="line"><span class="string">AI: Because artificial intelligence will help humans reach their full potential.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">New summary:</span></span><br><span class="line"><span class="string">The human inquires about the AI&#x27;s opinion on artificial intelligence. The AI believes that it is a force for good as it can help humans reach their full potential.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## Current summary</span></span><br><span class="line"><span class="string">&#123;summary&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## New lines of conversation</span></span><br><span class="line"><span class="string">&#123;new_lines&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## New summary</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    input_variables=SUMMARY_PROMPT.input_variables, <span class="comment"># input_variables为SUMMARY_PROMPT中的input_variables不变</span></span><br><span class="line">    template=prompt_template_str, <span class="comment"># template替换为上面重新编写的prompt_template_str</span></span><br><span class="line">)</span><br><span class="line">memory = ConversationSummaryBufferMemory(llm=llm, prompt=prompt, max_token_limit=<span class="number">60</span>)</span><br><span class="line"><span class="comment"># 添加历史memory，其中第一条SystemMessage为历史对话中Summary的内容，第二条HumanMessage和第三条AIMessage为历史对话中最后的对话内容</span></span><br><span class="line">memory.chat_memory.add_message(SystemMessage(content=<span class="string">&quot;The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human then asks the difference between python and golang in short. The AI responds that python is a high-level interpreted language with an emphasis on readability and code readability, while golang is a statically typed compiled language with a focus on concurrency and performance. Python is typically used for general-purpose programming, while golang is often used for building distributed systems.&quot;</span>))</span><br><span class="line">memory.chat_memory.add_user_message(<span class="string">&quot;Then if I want to build a distributed system, which language should I choose?&quot;</span>)</span><br><span class="line">memory.chat_memory.add_ai_message(<span class="string">&quot;If you want to build a distributed system, I would recommend golang as it is a statically typed compiled language that is designed to facilitate concurrency and performance.&quot;</span>)</span><br><span class="line"><span class="comment"># 调用memory.prune()确保chat_memory中的对话内容不超过max_token_limit</span></span><br><span class="line">memory.prune()</span><br><span class="line">conversation_with_summary = ConversationChain(</span><br><span class="line">    llm=llm,</span><br><span class="line">    <span class="comment"># We set a very low max_token_limit for the purposes of testing.</span></span><br><span class="line">    memory=memory,</span><br><span class="line">    verbose=<span class="literal">True</span>,</span><br><span class="line">)</span><br><span class="line"><span class="comment"># memory.prune()会在每次调用predict()后自动执行</span></span><br><span class="line">conversation_with_summary.predict(<span class="built_in">input</span>=<span class="string">&quot;Is there any well-known distributed system built with golang?&quot;</span>)</span><br><span class="line">conversation_with_summary.predict(<span class="built_in">input</span>=<span class="string">&quot;Is there a substitutes for Kubernetes in python?&quot;</span>)</span><br></pre></td></tr></table></figure><p>执行后verbose输出日志如下：</p><blockquote><p><strong>&gt; Entering new chain…</strong></p><p>Prompt after formatting:</p><p>***The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.</p><p>Current conversation:<br>System: The human asks the AI about its opinion on artificial intelligence and is told that it is a force for good that can help humans reach their full potential. The human then inquires about the differences between python and golang, with the AI explaining that python is a high-level interpreted language for general-purpose programming, while golang is a statically typed compiled language often used for building distributed systems.<br>Human: Then if I want to build a distributed system, which language should I choose?<br>AI: If you want to build a distributed system, I would recommend golang as it is a statically typed compiled language that is designed to facilitate concurrency and performance.<br>Human: Is there any well-known distributed system built with golang?<br>AI:***</p><p><strong>&gt; Finished chain.</strong></p><p><strong>&gt; Entering new chain…</strong></p><p>Prompt after formatting:</p><p>***The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.</p><p>Current conversation:<br>System: The human asks the AI about its opinion on artificial intelligence and is told that it is a force for good that can help humans reach their full potential. The human then inquires about the differences between python and golang, with the AI explaining that python is a high-level interpreted language for general-purpose programming, while golang is a statically typed compiled language designed to facilitate concurrency and performance, thus better suited for distributed systems. The AI recommends golang for building distributed systems.<br>Human: Is there any well-known distributed system built with golang?<br>AI: Yes, there are several well-known distributed systems built with golang. These include Kubernetes, Docker, and Consul.<br>Human: Is there a substitutes for Kubernetes in python?<br>AI:***</p><p><strong>&gt; Finished chain.</strong></p><p><em><strong>’Yes, there are several substitutes for Kubernetes in python. These include Dask, Apache Mesos and Marathon, and Apache Aurora.’</strong></em></p></blockquote><h2 id="agent"><a class="markdownIt-Anchor" href="#agent"></a> Agent</h2><p>在一些场景下，我们需要根据用户输入灵活地调用<code>LLM</code>和其它工具（<code>LangChain</code>将工具抽象为Tools这一组件），Agent为这样的应用程序提供了相关的支持</p><p><code>Agent</code>可以访问一套工具，并根据用户输入确定要使用<code>Chain</code>或是<code>Function</code>，我们可以简单的理解为他可以动态的帮我们选择和调用Chain或者已有的工具</p><p>常用的<code>Agent</code>类型如下：</p><h3 id="conversational-agent"><a class="markdownIt-Anchor" href="#conversational-agent"></a> Conversational Agent</h3><p>这类Agent可以根据Language Model的输出决定是否使用指定的Tool，以及使用什么Tool(这里的Tool也可以是一个Chain)，以及时的为Model I/O的过程补充信息</p><h3 id="openai-functions-agent"><a class="markdownIt-Anchor" href="#openai-functions-agent"></a> OpenAI functions Agent</h3><p>类似Conversational Agent，但它能够让Agent更进一步地帮忙提取指定Tool的参数等，甚至使用多个Tools</p><h3 id="plan-and-execute-agent"><a class="markdownIt-Anchor" href="#plan-and-execute-agent"></a> Plan and execute Agent</h3><p>抽象Agent“决定做什么”的过程为“planning what to do”和“executing the sub tasks”(这种方法来自<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2305.04091">&quot;Plan-and-Solve&quot;</a>这一篇论文)，其中“planning what to do”这一步通常完全由LLM完成，而“executing the sub tasks”这一任务则通常由更多的Tools来完成</p><h3 id="react-agent"><a class="markdownIt-Anchor" href="#react-agent"></a> ReAct Agent</h3><p>结合对LLM输出的归因和执行，类似OpenAI functions Agent，提供了一个更加明确的框架以及由论文支撑的方法</p><p><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171523369-b0c429.png" alt="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/08/17/202308171523369-b0c429.png"></p><h3 id="self-ask-with-search"><a class="markdownIt-Anchor" href="#self-ask-with-search"></a> <strong>Self ask with search</strong></h3><p>这类Agent会基于LLM的输出，自行调用Tools以及LLM来进行额外的搜索和自查，以达到拓展和优化输出的目的</p><h1 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> <strong>总结</strong></h1><p>LangChain 中的Data Connection将LLM与万物互联，给LangChain构建的应用带来了无限可能，而Agent又为应用开发中非常常见的”事件驱动“这一开发框架提供了偷懒的途径，将分支决策的工作交给LLM，这又进一步简化了应用开发的工作；结合基于高度抽象的Model I/O及Memory等组件，LangChain让开发者能够更快，更好更灵活地实现 LLM 助手、对话机器人等应用，极大地降低了 LLM 的使用门槛</p></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者:</span> <span class="post-copyright-info"><a href="mailto:undefined">Kevinello</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接:</span> <span class="post-copyright-info"><a href="http://kevinello.ltd/2023/09/06/LLM%E8%BF%9C%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AFChat%20Model%E2%80%94%E2%80%94LangChain%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/">http://kevinello.ltd/2023/09/06/LLM远不仅仅是Chat Model——LangChain基本概念与使用示例/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://kevinello.ltd" target="_blank">Kevinello</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/Prompt-Engineering/">Prompt-Engineering</a><a class="post-meta__tags" href="/tags/OpenAI/">OpenAI</a><a class="post-meta__tags" href="/tags/langchain/">langchain</a></div><div class="post_share"><div class="social-share" data-image="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071425723-3c7e69.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/01/01/%E6%88%90%E9%95%BF%E4%BA%8E%E7%84%A6%E8%99%91%E4%B8%8E%E6%8C%A3%E6%89%8E%E4%B8%AD%E2%80%94%E2%80%942023%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"><img class="prev-cover" src="https://kevinello-1302687393.file.myqcloud.com/picgo/2024/01/02/image-20240102133226569-405ca2.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">成长于焦虑与挣扎中——2023年终总结</div></div></a></div><div class="next-post pull-right"><a href="/2023/07/06/%E5%A6%82%E4%BD%95%E4%B8%BA%E7%A7%81%E6%9C%89%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BF%AB%E9%80%9F%E6%B2%89%E6%B7%80%E9%AB%98%E8%B4%A8%E9%87%8F%E6%95%B0%E6%8D%AE%E9%9B%86/"><img class="next-cover" src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071443770-70c1ed.png" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">如何为私有大语言模型快速沉淀高质量数据集</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/07/06/%E5%A6%82%E4%BD%95%E4%B8%BA%E7%A7%81%E6%9C%89%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BF%AB%E9%80%9F%E6%B2%89%E6%B7%80%E9%AB%98%E8%B4%A8%E9%87%8F%E6%95%B0%E6%8D%AE%E9%9B%86/" title="如何为私有大语言模型快速沉淀高质量数据集"><img class="cover" src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071443770-70c1ed.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-06</div><div class="title">如何为私有大语言模型快速沉淀高质量数据集</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i> <span>评论</span></div><div id="comment-switch"><span class="first-comment">Utterances</span><span class="switch-btn"></span><span class="second-comment">Twikoo</span></div></div><div class="comment-wrap"><div><div id="utterances-wrap"></div></div><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2022/04/11/myself-e3fde6.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info__name">Kevinello</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">51</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">97</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/kevinello"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/kevinello" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://space.bilibili.com/23149976" target="_blank" title="Bilibili"><i class="iconfont icon-bilibili-fill"></i></a><a class="social-icon" href="mailto:kevinello42@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#langchain%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">2.</span> <span class="toc-text">LangChain是什么</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#langchain%E7%9A%84%E4%B8%BB%E8%A6%81%E7%BB%84%E4%BB%B6"><span class="toc-number">3.</span> <span class="toc-text">LangChain的主要组件</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#model-io"><span class="toc-number">3.1.</span> <span class="toc-text">Model I&#x2F;O</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#language-model"><span class="toc-number">3.1.1.</span> <span class="toc-text">Language Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#prompts"><span class="toc-number">3.1.2.</span> <span class="toc-text">Prompts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#output-parser"><span class="toc-number">3.1.3.</span> <span class="toc-text">Output Parser</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">3.1.4.</span> <span class="toc-text">使用示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#data-connection"><span class="toc-number">3.2.</span> <span class="toc-text">Data connection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#document-loaders"><span class="toc-number">3.2.1.</span> <span class="toc-text">Document loaders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#document-transformers"><span class="toc-number">3.2.2.</span> <span class="toc-text">Document transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#vector-stores"><span class="toc-number">3.2.3.</span> <span class="toc-text">Vector stores</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#retrievers"><span class="toc-number">3.2.4.</span> <span class="toc-text">Retrievers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-2"><span class="toc-number">3.2.5.</span> <span class="toc-text">使用示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#chains"><span class="toc-number">3.3.</span> <span class="toc-text">Chains</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#router-chain"><span class="toc-number">3.3.1.</span> <span class="toc-text">Router Chain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sequential-chain"><span class="toc-number">3.3.2.</span> <span class="toc-text">Sequential Chain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#map-reduce-chain"><span class="toc-number">3.3.3.</span> <span class="toc-text">Map-reduce Chain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-3"><span class="toc-number">3.3.4.</span> <span class="toc-text">使用示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#memory"><span class="toc-number">3.4.</span> <span class="toc-text">Memory</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B-4"><span class="toc-number">3.4.1.</span> <span class="toc-text">使用示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#agent"><span class="toc-number">3.5.</span> <span class="toc-text">Agent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#conversational-agent"><span class="toc-number">3.5.1.</span> <span class="toc-text">Conversational Agent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#openai-functions-agent"><span class="toc-number">3.5.2.</span> <span class="toc-text">OpenAI functions Agent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#plan-and-execute-agent"><span class="toc-number">3.5.3.</span> <span class="toc-text">Plan and execute Agent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#react-agent"><span class="toc-number">3.5.4.</span> <span class="toc-text">ReAct Agent</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-ask-with-search"><span class="toc-number">3.5.5.</span> <span class="toc-text">Self ask with search</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/01/01/%E6%88%90%E9%95%BF%E4%BA%8E%E7%84%A6%E8%99%91%E4%B8%8E%E6%8C%A3%E6%89%8E%E4%B8%AD%E2%80%94%E2%80%942023%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/" title="成长于焦虑与挣扎中——2023年终总结"><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2024/01/02/image-20240102133226569-405ca2.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="成长于焦虑与挣扎中——2023年终总结"></a><div class="content"><a class="title" href="/2024/01/01/%E6%88%90%E9%95%BF%E4%BA%8E%E7%84%A6%E8%99%91%E4%B8%8E%E6%8C%A3%E6%89%8E%E4%B8%AD%E2%80%94%E2%80%942023%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/" title="成长于焦虑与挣扎中——2023年终总结">成长于焦虑与挣扎中——2023年终总结</a><time datetime="2024-01-01T04:31:34.000Z" title="发表于 2024-01-01 12:31:34">2024-01-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/09/06/LLM%E8%BF%9C%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AFChat%20Model%E2%80%94%E2%80%94LangChain%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/" title="LLM远不仅仅是Chat Model——LangChain基本概念与使用示例"><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071425723-3c7e69.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="LLM远不仅仅是Chat Model——LangChain基本概念与使用示例"></a><div class="content"><a class="title" href="/2023/09/06/LLM%E8%BF%9C%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AFChat%20Model%E2%80%94%E2%80%94LangChain%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%E4%B8%8E%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B/" title="LLM远不仅仅是Chat Model——LangChain基本概念与使用示例">LLM远不仅仅是Chat Model——LangChain基本概念与使用示例</a><time datetime="2023-09-05T16:08:35.000Z" title="发表于 2023-09-06 00:08:35">2023-09-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/07/06/%E5%A6%82%E4%BD%95%E4%B8%BA%E7%A7%81%E6%9C%89%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BF%AB%E9%80%9F%E6%B2%89%E6%B7%80%E9%AB%98%E8%B4%A8%E9%87%8F%E6%95%B0%E6%8D%AE%E9%9B%86/" title="如何为私有大语言模型快速沉淀高质量数据集"><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071443770-70c1ed.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="如何为私有大语言模型快速沉淀高质量数据集"></a><div class="content"><a class="title" href="/2023/07/06/%E5%A6%82%E4%BD%95%E4%B8%BA%E7%A7%81%E6%9C%89%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%BF%AB%E9%80%9F%E6%B2%89%E6%B7%80%E9%AB%98%E8%B4%A8%E9%87%8F%E6%95%B0%E6%8D%AE%E9%9B%86/" title="如何为私有大语言模型快速沉淀高质量数据集">如何为私有大语言模型快速沉淀高质量数据集</a><time datetime="2023-07-05T16:08:35.000Z" title="发表于 2023-07-06 00:08:35">2023-07-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/05/Redis%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85%EF%BC%9A%E5%BB%B6%E8%BF%9F%E9%97%AE%E9%A2%98%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/" title="Redis性能之巅：延迟问题排障指南"><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/02/28/202302281707283-c5d6b1.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Redis性能之巅：延迟问题排障指南"></a><div class="content"><a class="title" href="/2023/03/05/Redis%E6%80%A7%E8%83%BD%E4%B9%8B%E5%B7%85%EF%BC%9A%E5%BB%B6%E8%BF%9F%E9%97%AE%E9%A2%98%E6%8E%92%E9%9A%9C%E6%8C%87%E5%8D%97/" title="Redis性能之巅：延迟问题排障指南">Redis性能之巅：延迟问题排障指南</a><time datetime="2023-03-05T09:08:07.000Z" title="发表于 2023-03-05 17:08:07">2023-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/05/clash-on-linux%E9%85%8D%E7%BD%AE/" title="clash-on-linux配置"><img src="https://kevinello-1302687393.file.myqcloud.com/picgo/2023/03/05/202303051704184-34cc7e.png" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="clash-on-linux配置"></a><div class="content"><a class="title" href="/2023/03/05/clash-on-linux%E9%85%8D%E7%BD%AE/" title="clash-on-linux配置">clash-on-linux配置</a><time datetime="2023-03-05T07:34:11.000Z" title="发表于 2023-03-05 15:34:11">2023-03-05</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(https://kevinello-1302687393.file.myqcloud.com/picgo/2023/09/07/202309071425723-3c7e69.png)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Kevinello</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">🥳 🥳 🥳 🥳 🥳</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Algolia</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch@4/dist/algoliasearch-lite.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js"></script><script src="/js/search/optimized_algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function loadUtterances () {
  let ele = document.createElement('script')
  ele.setAttribute('id', 'utterances_comment')
  ele.setAttribute('src', 'https://utteranc.es/client.js')
  ele.setAttribute('repo', 'Kevinello/gitalk')
  ele.setAttribute('issue-term', 'pathname')
  let nowTheme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'photon-dark' : 'github-light'
  ele.setAttribute('theme', nowTheme)
  ele.setAttribute('crossorigin', 'anonymous')
  ele.setAttribute('async', 'true')
  document.getElementById('utterances-wrap').insertAdjacentElement('afterbegin',ele)
}

function utterancesTheme () {
  const iframe = document.querySelector('.utterances-frame')
  if (iframe) {
    const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'photon-dark' : 'github-light'
    const message = {
      type: 'set-theme',
      theme: theme
    };
    iframe.contentWindow.postMessage(message, 'https://utteranc.es');
  }
}

if ('Utterances' === 'Utterances' || !false) {
  if (false) btf.loadComment(document.getElementById('utterances-wrap'), loadUtterances)
  else loadUtterances()
} else {
  function loadOtherComment () {
    loadUtterances()
  }
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'blog-comments-9gil6as164013b6c',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.vemoji)'))
      }
    }, null))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'blog-comments-9gil6as164013b6c',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      document.getElementById('twikoo-count').innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Utterances' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async data-pjax src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>